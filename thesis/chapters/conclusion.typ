#import "../lib.typ": *

= Conclusion
In this thesis, we have conducted an exploratory analysis of the explainable artificial intelligence (XAI) field through generation of counterfactual explanations for $k$-means clustering. We have surveyed the field, finding this specific subject to contain very little current research. In contrast, the field of supervised counterfactual generation contains well-established methods like DiCE @mothilalExplainingMachineLearning2020 and BayCon @romashovBayConModelagnosticBayesian2022, which we have used as baselines by adapting it to an unsupervised setting.

We have identified only two existing methods for generating counterfactuals for clusterings. The first, presented in @spagnolCounterfactualExplanationsClustering2024, is an adaptation of BayCon to use a soft scoring mechanism for guiding the search instead of the binary metric used in the supervised version. The second method, CFClust @vardakasCounterfactualExplanationsKmeans2025, instead defines a simple objective which can be solved using non-iterative methods. 

Given this gap in the existing literature, the focus of this thesis has been the contribution of 4 novel methods which use a variety of approaches for counterfactual generation. The first method, GradCF, is a simple approach for generating a single counterfactual by running gradient descent on an objective function. The next methods, CFAE and NeCS, use an iterative heuristic-based approach for finding multiple counterfactuals. CFAE uses random sampling from the target cluster to approach the cluster boundary, while NeCS uses a given instance's $k$-nearest neighbors in the target cluster in order to generate $k$ diverse explanations. Finally, TGCF is a tree-based method which uses existing decision tree algorithms, and generates counterfactuals by changing $x$ based on the thresholds.

We have conducted detailed experiments on 4 datasets in order to analyze the performance of the novel methods in relation to the baselines and existing methods. Using 100 random problem instances for each dataset, we have reported a wide range of metrics and analyzed the results. We find our contributed methods to generally outperform the existing ones, especially in the _%Explained_ metric, since our methods are able to explain a wider variety of instances. For cases where all methods are able to generate valid counterfactuals, the performance is more comparable. We find GradCF to perform consistently well across most metrics, especially _plausibility_, although it has the major drawback that it only generates a single counterfactual. CFAE works very well for generating counterfactuals close to the cluster boundary, indicated by its superior _dissimilarity_ score. NeCS achieves its goal of getting a high _sparsity_ and _diversity_ for the generated counterfactuals, while keeping comparable performance for other metrics. Lastly, TGCF manages to generate a good set of diverse counterfactuals, with similar performance in other metrics. However, it does not transfer well to higher dimensions, likely due to the complex tree required to capture the clustering.

In conclusion, this thesis has contributed 4 new approaches for counterfactual generation in a clustering setting. Although they can all be improved and generalized to other settings as outlined in @sec:future_work, we find them to perform well compared to existing methods.