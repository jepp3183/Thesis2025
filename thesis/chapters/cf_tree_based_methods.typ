== Novel Tree-Based Counterfactual Methods<tree_methods>
Another venue of design for generating counterfactuals for unsupervised tasks could be implemented by exploiting information contained in decision tree approximations of the clustering task. This idea of utilizing decision trees for explanations in a post-hoc manner comes from methods such as @cravenExtractingTreeStructuredRepresentations1995, who highlight that decision trees functions as intuitive explanation models, as they reveal decision logic implemented by the black box model. This enables decision trees to be used as an appropriate surrogate model for our black box. These surrogate decision trees enable an analyzer to move along the tree to reason about the clustering or classification of a certain data instance. This in turn enables an analyst to reason about possible counterfactual changes for an alternative outcome.

=== Our novel model (name) / alternative title
To solve the task of generating valid and intuitive counterfactuals, we have made a novel method based on this class of tree-based explanation models. Our proposed method is model-agnostic in that it works with any clustering model and that any kind of surrogate decision tree model can be employed. Our method furthermore reveals intuitive counterfactual changes as a usual decision tree partitions the data based on simple if-else rules. Though one thing to keep in mind when using these kinds of models, is that decision trees can have a fidelity/complexity trade-off as we often have to employ deeper trees to generate a more faithful surrogate model.

To generate counterfactuals using (name---------------), we will first need an instance to be explained and a target cluster. The next step of the algorithm is then to train our choice of a surrogate decision tree model for our task. We chose two different decision tree methods to be able to compare their results and also to show how the algorithm functions with alternative surrogate models. The first decision tree model that we will be using is Sci-Kit Learns Decision Tree Classifier(DCT)#footnote[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html], which we will train on the cluster labels. One problem with this specific surrogate model is that it introduces a hyperparameter "min_impurity_decrease", which will impact the fidelity of the trained surrogate model. The second model that we will be using is the Iterative Mistake Minimization (IMM) algorithm introduced in @dasguptaExplainableKMeansKMedians2020. In @dasguptaExplainableKMeansKMedians2020, they work with the task of creating simple explanations of clusters using small decision trees. These decision trees will have k leafs, one for each cluster and a depth of at most $k-1$. In each iteration of the algorithm it aims to create a cut or decision boundary to minimize the amount of points that are not categorized into the same leaf as its cluster center. This approach will result in few and simple feature thresholds, which will help in intuitive understanding of cluster assignments. An example of an explanation of the IMM algorithm can be seen on @fig:imm_example.

#figure(
  image("../assets/imm_example.png"),
  caption: [Example of IMM threshold tree explanation from @dasguptaExplainableKMeansKMedians2020, where each cluster is assigned to its own leaf]
)<fig:imm_example>

After a surrogate model has been chosen and fitted to the unsupervised task at hand, the next step is to find the tree path to our explained instance and the path to each leaf representing the target cluster. For the IMM tree we will only have a single target leaf as each cluster will be assigned only to one leaf, but for the DCT we can have an arbitrary amount of target leafs based on how accurate we want our model to be. 

After all paths have been calculated we want to find the deepest node in the tree for each target leaf that is a parent for both that target and the instance leaf. From the deepest common parent node in the tree for any given target leaf, we know that the target and the instance must have the taken the same path in the tree to this node and therefore already share the necessary feature values for the decision thresholds. This aforementioned fact ensures that we only have to change our instance based on the feature thresholds evaluated from this common parent on the path to the target leaf. To put this fact into perspective we can again look at the visualization on @fig:imm_example subplot (c), where if we want to go from an instance in cluster 3 to target cluster 4, we will only have to change our instance based on the thresholds: $y <= -4$ and $y<= 4$, which results in a y-value in the interval $[-4,4)$. As we only change our instance's features depending on the feature threshold on the path to the target leaf we are also ensured that we will get a classification of the counterfactual in the target class, at least in respect to the surrogate model. Though this will not always ensure a valid counterfactual when evaluating the cluster assignment. After the procedure has been applied to every target leaf in the surrogate model, we will have a candidate counterfactual for each of these leafs. 

On @fig:dtc_vs_imm we can see (name---------------) using each of the proposed surrogate models, each model has precisely 5 leafs, one for each cluster, which can be obtained by DTC because of a relatively simple 2-dimensional synthetic data. In the presented example the target cluster is 2, which is also the predicted cluster when predicting using the surrogate model. Though it is also quite easily observed that these counterfactuals are not valid when evaluated subject to the actual cluster labels. To solve this we added a post-processing step to achieve a higher robustness for the found counterfactuals. We call these more robust counterfactuals $C'$, which can also be se on @fig:dtc_vs_imm. The robustness and change of our counterfactuals are reliant on the hyperparameter "robustness_factor", which takes values between 0 and 1, signifying how much the algorithm should change previously permuted features towards the target centroid in that dimension.

#figure(
  image("../assets/dtc_vs_imm.png"),
  caption: [Comparison of (name----------------) using surrogate model Decision Tree Classifier#footnote[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html] and IMM @dasguptaExplainableKMeansKMedians2020, with C' generated using a robustness factor of 0.7]
)<fig:dtc_vs_imm>

The complexity of this algorithm is very much dependant on the complexity or fidelity of the chosen surrogate model. For the IMM algorithm we know that the tree can have a depth of at most $k-1$, which induces at most $k-1$ changes to achieve a counterfactual, though the amount of changes would only be $log(k)$ for a balanced tree. For the Decision Tree Classifier we cannot say anything specific about the complexity as this is again very dependant on the depth of the tree and also the amount of target leafs that we will have to run the algorithm on.

=== Could run on forest/ensemble of trees




