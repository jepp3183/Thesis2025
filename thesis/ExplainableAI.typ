= Explainable AI
//TODO: chapter intro? explain goal of chapter, focus of thesis etc.?

The field of machine learning (ML) has made great strides in recent years, resulting in much better classification accuracy and more complex models with potentially millions of parameters. Given the impressive performance of these models, industry has been quick to adopt them into their AI systems in production. 

However, these models are often used as black boxes: a tool which, when given an input, produces an output with no explanation or justification of how or why the model returned the given result. Furthermore, these advanced models have been proven to work in critical fields like cancer diagnosis or bank loan approval @bhattExplainableMachineLearning2020 @estevaDermatologistlevelClassificationSkin2017, both of which require a great deal of trust in the model in order to satisfy both end-users and stakeholders. For example, patients confronted with an AI-driven assessment of potentially malignant lesions would reasonably expect either an explanation of the model's conclusion or validation from a medical professional. Additionally, explainable AI (XAI) can also help combat bias and discrimination caused by AI systems, by investigating if the model is making decision based primarily on one or more sensitive attributes like age, gender or race. 

The problem of black-box AI systems has also recently led to legislation being passed by the EU as part of the AI act @EU_AI_Act_Article_86. In the legislation, article 86 ensures the "right to explanation" for individuals in the EU, when affected by the decision of a high-risk AI system, defined as systems acting in the area of biometrics, education, employment, law enforcement, etc. Here, the right to explanation means the affected subject can obtain a clear explanation of how AI was used in the decision, and the main points contributing to the decision. When the article goes into effect in 2026, deployers of AI systems will likely need to depend on XAI methods to comply with the new regulations.

Finally, XAI is also interesting from a designer's perspective. By understanding how a model works, designers can better understand the strengths and weaknesses of their models, and potentially improve them. For example, one can use gradient-based explanation methods like Grad-CAM @selvarajuGradCAMVisualExplanations2020 for convolutional neural networks in order to understand which areas of an image contributed most to a classification (@fig:grad_cam_example). In combination with domain knowledge, a designer can quickly use this to diagnose issues with a model. 
#figure(
    image("./assets/gradcam_example.png", width: 75%),
    caption: [Example of an explanation generated by Grad-CAM @selvarajuGradCAMVisualExplanations2020],
)<fig:grad_cam_example>

== Properties of Explanations <sec:properties_of_explanations>
There exists a large number of ways to explain ML models. These explanations can be broadly categorised using the following categories, based loosely on the taxonomy from @angelovExplainableArtificialIntelligence2021.

*Local vs. global explanations*. When seeking an explanation, one can either try to explain the entire model globally, or one can explain the model locally based on its behaviour when given a single instance. As an example, the prediction given by Grad-CAM in @fig:grad_cam_example is a local explanation of an instance. The well-known LIME feature-attribution method @ribeiroWhyShouldTrust2016 provides a form of global perspective by picking a limited number of instances along with their explanations based on a submodular objective function that optimizes for large attribution scores over as many features as possible while minimizing redundancy of explanations.

*Model-specific vs. model-agnostic explanations*. An explanation method may place certain requirements on the model being explained, meaning it does not treat it as a true "black box". For example, gradient-based methods like Grad-CAM naturally require a differentiable model. Other methods like LIME place no requirements on the model to be explained, only needing access to a function $f$ which returns a prediction when given an instance.

*Post-hoc vs. intrinsic explanations*. A post-hoc explanation method generates explanations on an existing, trained model, whereas others may instead produce a model based on a combined objective of both achieving high accuracy, while also ensuring the final models is explainable. In this context, there is often an accuracy/explainability trade-off, since explainability often amounts to having a model that is simple enough for human practitioners to understand. In the literature, such models are also described as transparent (as opposed to black-box). As an example, the iterative mistake minimization method presented in @dasguptaExplainableKMeansKMedians2020 creates a global clustering explanation by approximating reference clusters using threshold trees. The trade-off then arises because the tree should be small for explainability, whereas a larger tree would better approximate the reference clusters.

*Exogenous vs. endogenous explanations*. In the context of counterfactual explanations, which we introduce in detail in @sec:counterfactual_explanations, an explanation is a single point from the same domain as the instance being explained. In such cases, the point may either be a point picked from the dataset (endogenous), or it can be a novel point generated by the explanation method (exogenous). Although most methods generate exogenous explanations, endogenous methods offer a more natural guarantee that the explanations are realistic or plausible in relation to the existing data.

== Counterfactual Explanations <sec:counterfactual_explanations>
The focus of this thesis is placed primarily on a specific type of explanation: counterfactuals. In this section, we introduce the intuition behind them, along with basic notation used throughout the thesis. 

Counterfactual explanations answer the question of "what should be done to change this outcome?". Thus, they are a form of local explanation, always explaining only a single given instance $x$. The typical example used in literature is based on binary classification, where a model $f$ outputs $1$ if a given instance representing a loan application was accepted for a loan, and outputs $0$ if it was denied (see @fig:CF_example). In this case, given some instance $x in RR^d$ where $f(x)=0$ a counterfactual explanation is simply a different instance $x' in RR^d$ s.t. $f(x')=1$ i.e. a different application which was accepted for a loan. This type of explanation offers an intuitive way for users to understand what should be done in order to change a prediction. In the example above, the counterfactual application $x'$ might have a higher income than the original application $x$, which helps in identifying cause-effect relations in the underlying model. This type of explanation is very intuitive to people according to the cognitive psychology literature @byrneCounterfactualsExplainableArtificial2019. If done well, counterfactuals can also serve as a very useful explanation for practitioners, allowing them to understand which features might be important for a model to output a certain result. For example, if too many counterfactuals suggest changing sensitive attributes, the model may be too biased to use in practice.

#figure(
    image("./assets/CF_example.png", width: 50%),
    caption: [Simple visual example of a counterfactual explanation. ],
)<fig:CF_example>

Of course, there is no single criteria for what makes a good counterfactual. However, based on the literature, we present a handful of properties that might make sense to consider when designing counterfactual explainers @guidottiCounterfactualExplanationsHow2024. First, and most importantly, a counterfactual should be valid, meaning it should lead to a different prediction than the original i.e. $f(x) eq.not f(x')$. Secondly, the counterfactual should present a _minimal_ change to the original instance. This can be represented using some form of similarity measure between the two points $x "and" x'$, but many methods also take into account the sparsity of the counterfactual i.e. how many features are different between $x$ and $x'$. A sparse counterfactual is much easier for people to reason about, and might be preferable to one with higher similarity, but lower sparsity. Aside from the three properties above, it can also be beneficial to consider if the counterfactual is an outlier based on the existing points in the target class. We call this property plausibility. Lastly, some methods are also designed to return multiple counterfactuals for a single instance. In such cases, it is reasonable to consider the diversity of the counterfactuals returned by the method, since multiple highly similar counterfactuals offer no additional value for a practitioner.

// Counterfactual usage for medicare application form error correction:@bhattExplainableMachineLearning2020


// fairness: select set of immutable features

== Unsupervised Explanations

Although explainable AI is most often associated with supervised learning tasks like classification, it can also be used for unsupervised tasks where clustering is often the main objective. In general however, unsupervised explainability is less well-studied than supervised methods. But there has been some recent interest in the field of clustering explainability. The iterative mistake minimization mentioned in @sec:properties_of_explanations @dasguptaExplainableKMeansKMedians2020 is one such example where the goal is to offer an explainable clustering using simple, human-interpretable rules to decide to which cluster a point belongs. In @ellisAlgorithmAgnosticExplainabilityUnsupervised2021, they present an agnostic method to explain any given clustering using feature importance found by permuting the data features and seeing how much the clustering changes.

Due to the limited amount of research in this area, this thesis will focus primarily on studying and implementing methods for unsupervised explanations. Specifically, we focus on counterfactual explanations as detailed in @sec:counterfactual_explanations applied to unsupervised clustering. This is a specialized area with few existing methods. We explore existing literature and methods in @sec:unsupervised_counterfactual_methods.
