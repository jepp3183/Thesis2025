#import "../lib.typ": *

= Explainable AI
//TODO: chapter intro? explain goal of chapter, focus of thesis etc.?
//maybe outline some different types of explanations? feature importance, gradients, etc?

The field of machine learning (ML) has made great strides in recent years, resulting in much better classification accuracy and more complex models with potentially millions of parameters. Given the impressive performance of these models, industry has been quick to adopt them into their AI systems in production.

However, these models are often used as black boxes: a tool which, when given an input, produces an output with no explanation or justification of how or why the model returned the given result. Furthermore, these advanced models work in critical fields like cancer diagnosis or bank loan approval @bhattExplainableMachineLearning2020 @estevaDermatologistlevelClassificationSkin2017, both of which require trust in the model in order to satisfy stakeholders. For example, patients confronted with an AI-driven assessment of potentially malignant lesions would reasonably expect either an explanation of the model's conclusion or validation from a medical professional. This problem has given rise to the field of explainable AI (XAI), which is concerned with "opening the black box" by developing methods that can explain the decisions made by these models. Additionally, XAI can also help combat bias and discrimination caused by AI systems, by investigating if the model is making decisions based primarily on one or more sensitive attributes like age, gender or race. 

The problem of black-box AI systems has also recently led to legislation by the EU as part of the AI act @EU_AI_Act_Article_86. In the legislation, article 86 ensures the "right to explanation" for individuals in the EU, when affected by the decision of a high-risk AI system, defined as systems acting in the area of biometrics, education, employment, law enforcement, etc. Here, the right to explanation means the affected subject can obtain an explanation of how AI was used in the decision, and the main factors contributing to the decision. When the article goes into effect in 2026, deployers of AI systems will likely need XAI methods to comply with the new regulations.

Finally, XAI is also interesting from a developer's perspective. By understanding how a model works, designers can better understand the strengths and weaknesses of their models, and potentially improve them. For example, one can use gradient-based explanation methods like Grad-CAM @selvarajuGradCAMVisualExplanations2020 for convolutional neural networks in order to understand which areas of an image contributed most to a classification (@fig:grad_cam_example). In combination with domain knowledge, a developer can quickly use this to diagnose issues with a model. 
#figure(
    image("../assets/gradcam_example.png", width: 75%),
    caption: [Example of an explanation generated by Grad-CAM @selvarajuGradCAMVisualExplanations2020],
)<fig:grad_cam_example>

== Properties of Explanations <sec:properties_of_explanations>
There exists a large number of ways to explain ML models. These explanations can be broadly categorised using the following categories, based loosely on the taxonomy from @angelovExplainableArtificialIntelligence2021.

*Local vs. global explanations*. When seeking an explanation, one can either try to explain the entire model globally, or one can explain the model locally given a single input. As an example, the prediction given by Grad-CAM in @fig:grad_cam_example is a local explanation of an instance, where it highlight which parts of the image have had the greatest effect on the prediction. Alternatively, the well-known LIME feature-attribution method @ribeiroWhyShouldTrust2016 provides primarily local explanations, but also offers global perspective by picking a limited number of instances along with their explanations while minimizing redundancy. // Better example of some global method?

*Model-specific vs. model-agnostic explanations*. An explanation method may place certain requirements on the model being explained, meaning it does not treat it as a true "black box". For example, gradient-based methods like Grad-CAM require a differentiable model. Other methods like LIME place no requirements on the model to be explained, only needing access to a function $f$ which returns a prediction when given an instance. Although model-agnostic methods are desirable due to their flexibility, model-specific methods are able to exploit the properties of certain models to achieve improvements in performance. 

*Post-hoc vs. intrinsic explanations*. A post-hoc explanation method generates explanations for an existing, trained model, whereas intrinsic ones instead produce a model based on a combined objective of achieving high accuracy while also ensuring the final model is explainable. In this context, there is often an accuracy/explainability trade-off, since explainability often amounts to having a model that is simple enough for humans to understand. In the literature, such models are also described as transparent (as opposed to black-box). As an example, the iterative mistake minimization method presented in @dasguptaExplainableKMeansKMedians2020 creates a global clustering explanation by approximating reference clusters using threshold trees. The trade-off then arises because the tree should be small for explainability, whereas a larger tree is able to better approximate the reference clusters.

*Exogenous vs. endogenous explanations*. In the context of counterfactual explanations, which we introduce in detail in @sec:counterfactual_explanations, an explanation is a single point from the same domain as the instance being explained. In such cases, the point may either be a point picked from the dataset (endogenous), or it can be a novel point generated by the explanation method (exogenous). Although most methods generate exogenous explanations, endogenous methods offer a natural guarantee that the explanations are realistic or plausible in relation to the existing data.

== Counterfactual Explanations <sec:counterfactual_explanations>
The focus of this thesis is placed primarily on a specific type of explanation: counterfactuals. In this section, we introduce the intuition behind them, along with basic notation used throughout the thesis. 

Counterfactual explanations answer the question of "what should be done to change this outcome?". Thus, they are a form of local explanation, always explaining only a single given instance $x$. The typical example used in literature is based on binary classification, where a model $f$ outputs $1$ if a given instance representing a loan application was accepted for a loan, and outputs $0$ if it was denied (see @fig:CF_example). In this case, given some instance $x in RR^d$ where $f(x)=0$ a counterfactual explanation is a different instance $x' in RR^d$ s.t. $f(x')=1$ with the additional requirement that $x'$ should be close to $x$. Thus, the counterfactual would be a different, but similar, application which was accepted for a loan. This type of explanation offers an intuitive way for users to understand what should be done in order to change a prediction. In the example above, the counterfactual application $x'$ might have a higher income than the original application $x$, which helps in identifying cause-effect relations in the underlying model. This type of explanation is intuitive to people according to the cognitive psychology literature @byrneCounterfactualsExplainableArtificial2019. Counterfactuals can also serve as a useful explanation for practitioners, allowing them to understand which features might be important for a model to output a certain result. For example, if several counterfactuals suggest changing sensitive attributes, the model may be overly biased and must be adjusted before a real-world deployment.

#figure(
    image("../assets/CF_example.png", width: 50%),
    caption: [Simple visual example of a counterfactual explanation. ],
)<fig:CF_example>

When evaluating such explanations, there is no single criterion for what makes a good counterfactual. However, based on the literature, we present some properties that one might consider when designing and evaluating counterfactual explainers @guidottiCounterfactualExplanationsHow2024. First, and most importantly, a counterfactual should be valid, meaning it should lead to a different prediction than the original i.e. $f(x) eq.not f(x')$. Secondly, the counterfactual should present a _minimal_ change to the original instance. This can be represented using a simple similarity measure between the two points $x "and" x'$, but many methods also take into account the sparsity of the counterfactual i.e. how many features are identical between $x$ and $x'$. A sparse counterfactual is much easier for people to reason about, and might be preferable to one with higher similarity, but lower sparsity. 

Aside from the three properties above, it can also be beneficial to consider if the counterfactual is an outlier based on the existing points in the target class or cluster, since a cluster can easily contain points that lie far away from the points it contains, especially for centroid-based methods like k-means. We call this property plausibility, and it can be computed using outlier scoring algorithms like LOF, which we use for our evaluation @breunigLOFIdentifyingDensitybased2000. Finally, some methods are also designed to return multiple counterfactuals for a single instance. In such cases, it is reasonable to consider the diversity of the counterfactuals returned by the method, since multiple highly similar counterfactuals offer no additional value for a practitioner.

// Counterfactual usage for medicare application form error correction:@bhattExplainableMachineLearning2020


// fairness: select set of immutable features

== Unsupervised Explanations

Although explainable AI is most frequently associated with supervised learning tasks like classification, it can also be used for unsupervised tasks where clustering is often the main objective. In general however, unsupervised explainability is less studied than supervised methods, although there has been some recent interest in the field of clustering explainability. The iterative mistake minimization mentioned in @sec:properties_of_explanations @dasguptaExplainableKMeansKMedians2020 is one such example where the goal is to offer an explainable clustering using simple, human-interpretable rules to describe to which cluster a point belongs. In @ellisAlgorithmAgnosticExplainabilityUnsupervised2021, they present an agnostic method to explain any given clustering using feature importance found by permuting the data features and noting how much the clustering changes.

Due to the limited amount of research in this area, this thesis will focus primarily on studying and implementing methods for unsupervised explanations. Specifically, we focus on counterfactual explanations as detailed in @sec:counterfactual_explanations applied to unsupervised centroid-based clusterings. This is a specialized area with few existing methods and literature, most of which we explore in @sec:unsupervised_counterfactual_methods. When studying counterfactuals for clusterings, we define a counterfactual using an instance $x$ belonging to some cluster, along with a target cluster for the counterfactual $x'$. 

Counterfactuals for clusterings have many potential real-world use cases. For example, when applied to typical clustering tasks like customer segmentation or healthcare patient stratification, it can supply realistic and actionable ways to move a "low-value" customer to a more "high-value" cluster, or it can suggest lifestyle changes or treatments to move patients to a "healthy" cluster. Aside from this, as mentioned in @sec:counterfactual_explanations, counterfactuals are also useful simply because they offer a human-interpretable way for designers to understand their model or clustering.
