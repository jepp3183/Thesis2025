@article{angelovExplainableArtificialIntelligence2021,
  title = {Explainable Artificial Intelligence: An Analytical Review},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Angelov, Plamen P. and Soares, Eduardo A. and Jiang, Richard and Arnold, Nicholas I. and Atkinson, Peter M.},
  year = {2021},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {11},
  number = {5},
  pages = {e1424},
  issn = {1942-4795},
  doi = {10.1002/widm.1424},
  urldate = {2025-03-05},
  abstract = {This paper provides a brief analytical review of the current state-of-the-art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested. This article is categorized under: Technologies {$>$} Artificial Intelligence Fundamental Concepts of Data and Knowledge {$>$} Explainable AI},
  langid = {english},
  keywords = {black-box models,deep learning,explainable AI,machine learning,prototype-based models,surrogate models},
  file = {/home/jeppe/Zotero/storage/YX98RMPV/Angelov et al. - 2021 - Explainable artificial intelligence an analytical review.pdf;/home/jeppe/Zotero/storage/3U5AZGCG/widm.html}
}

@inproceedings{angiulliCounterfactualsExplanationsOutliers2023,
  title = {Counterfactuals {{Explanations}} for {{Outliers}} via {{Subspaces Density Contrastive Loss}}},
  booktitle = {International {{Conference}} on {{Discovery Science}}},
  author = {Angiulli, Fabrizio and Fassetti, Fabio and Nistic{\'o}, Simona and Palopoli, Luigi},
  year = {2023},
  pages = {159--173},
  publisher = {Springer}
}

@article{arthurKmeansAdvantagesCareful,
  title = {K-Means++: {{The Advantages}} of {{Careful Seeding}}},
  author = {Arthur, David and Vassilvitskii, Sergei},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is O(log k)-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/YFK27HE5/Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf}
}

@article{bentleyMultidimensionalBinarySearch1975,
  title = {Multidimensional Binary Search Trees Used for Associative Searching},
  author = {Bentley, Jon Louis},
  year = {1975},
  month = sep,
  journal = {Communications of the ACM},
  volume = {18},
  number = {9},
  pages = {509--517},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/361002.361007},
  urldate = {2025-03-05},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/DZWMR5DJ/Bentley - 1975 - Multidimensional binary search trees used for associative searching.pdf}
}

@inproceedings{bhattExplainableMachineLearning2020,
  title = {Explainable Machine Learning in Deployment},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} M. F. and Eckersley, Peter},
  year = {2020},
  month = jan,
  pages = {648--657},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3351095.3375624},
  urldate = {2025-03-03},
  abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/XQ4XPKLT/Bhatt et al. - 2020 - Explainable machine learning in deployment.pdf}
}

@inproceedings{breunigLOFIdentifyingDensitybased2000,
  title = {{{LOF}}: Identifying Density-Based Local Outliers},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  year = {2000},
  pages = {93--104}
}

@inproceedings{byrneCounterfactualsExplainableArtificial2019,
  title = {Counterfactuals in {{Explainable Artificial Intelligence}} ({{XAI}}): {{Evidence}} from {{Human Reasoning}}},
  shorttitle = {Counterfactuals in {{Explainable Artificial Intelligence}} ({{XAI}})},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Byrne, Ruth M. J.},
  year = {2019},
  month = aug,
  pages = {6276--6282},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macao, China},
  doi = {10.24963/ijcai.2019/876},
  urldate = {2025-03-07},
  abstract = {Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI).  Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/6B5QKH2E/Byrne - 2019 - Counterfactuals in Explainable Artificial Intelligence (XAI) Evidence from Human Reasoning.pdf}
}

@inproceedings{chawlaKmeansUnifiedApproach2013,
  title = {K-Means--: {{A Unified Approach}} to {{Clustering}} and {{Outlier Detection}}},
  booktitle = {Proceedings of the 13th {{SIAM International Conference}} on {{Data Mining}}, {{May}} 2-4, 2013. {{Austin}}, {{Texas}}, {{USA}}},
  author = {Chawla, Sanjay and Gionis, Aristides},
  year = {2013},
  pages = {189--197},
  publisher = {SIAM},
  doi = {10.1137/1.9781611972832.21}
}

@article{dasguptaExplainableKMeansKMedians,
  title = {Explainable K-{{Means}} and k-{{Medians Clustering}}},
  author = {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal and Rashtchian, Cyrus},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/IX98EYLP/Dasgupta et al. - Explainable k-Means and k-Medians Clustering.pdf}
}

@inproceedings{dasilvaUsingKassociatedOptimal2022,
  title = {Using the {{K-associated Optimal Graph}} to {{Provide Counterfactual Explanations}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Fuzzy Systems}} ({{FUZZ-IEEE}})},
  author = {{da Silva}, Ariel Tadeu and Bertini, Jo{\~a}o Roberto},
  year = {2022},
  month = jul,
  pages = {1--8},
  issn = {1558-4739},
  doi = {10.1109/FUZZ-IEEE55066.2022.9882751},
  urldate = {2025-02-07},
  abstract = {Only recently have data mining results been thought to aid human interpretability. Explanations are useful to understand the reasons why (or why not) the model has (or hasn't) achieved a given decision. Counterfactual explanations aim to explain why not the model yield an expected decision. A counterfactual explanation is usually done by a post-hoc algorithm which often requires access to training data or model details. Also, the majority of such algorithms do not generate robust explanations, once they assume the model is noise-free and will not be updated over time. This paper proposes a model-agnostic, training data-independent algorithm to provide robust counterfactual explanations. The proposed method generates data samples around the instance to be explained and builds a K-Associated Optimal Graph (KAOG) with those data. KAOG allows measuring how intertwined the data examples are regarding their classes. This way, the explanation method can search for an example that relies on a noise-free area in the attribute space, granting trust to the explanation. Experiment results on counterfactual feasibility and distance from query data show the effectiveness of the proposed algorithm when compared to ten state-of-the-art methods on three data sets.},
  keywords = {Data mining,Data models,Fuzzy systems,Robustness,Training,Training data},
  file = {/home/jeppe/Zotero/storage/FJY4S65Q/da Silva and Bertini - 2022 - Using the K-associated Optimal Graph to Provide Counterfactual Explanations.pdf;/home/jeppe/Zotero/storage/L5JFQK2Q/9882751.html}
}

@inproceedings{esterDensityBasedAlgorithmDiscovering1996,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} ({{KDD-96}}), {{Portland}}, {{Oregon}}, {{USA}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  editor = {Simoudis, Evangelos and Han, Jiawei and Fayyad, Usama M.},
  year = {1996},
  pages = {226--231},
  publisher = {AAAI Press}
}

@article{estevaDermatologistlevelClassificationSkin2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  year = {2017},
  month = feb,
  journal = {Nature},
  volume = {542},
  number = {7639},
  pages = {115--118},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature21056},
  urldate = {2025-03-03},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/ZLWAMJ3V/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf}
}

@article{EU_AI_Act_Article_86,
  title = {Article 86: {{Right}} to Explanation of Individual Decision-Making},
  journal = {EU Artificial Intelligence Act}
}

@misc{frazierTutorialBayesianOptimization2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02811},
  eprint = {1807.02811},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.02811},
  urldate = {2025-02-24},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/ZMH6FHZK/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf}
}

@article{guidottiCounterfactualExplanationsHow2024,
  title = {Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking},
  author = {Guidotti, Riccardo},
  year = {2024},
  journal = {Data Mining and Knowledge Discovery},
  volume = {38},
  number = {5},
  pages = {2770--2824},
  publisher = {Springer}
}

@inproceedings{hanAchievingCounterfactualFairness2023,
  title = {Achieving Counterfactual Fairness for Anomaly Detection},
  booktitle = {Pacific-{{Asia Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Han, Xiao and Zhang, Lu and Wu, Yongkai and Yuan, Shuhan},
  year = {2023},
  pages = {55--66},
  publisher = {Springer}
}

@inproceedings{jinRankingOutliersUsing2006,
  title = {Ranking {{Outliers Using Symmetric Neighborhood Relationship}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}, 10th {{Pacific-Asia Conference}}, {{PAKDD}} 2006, {{Singapore}}, {{April}} 9-12, 2006, {{Proceedings}}},
  author = {Jin, Wen and Tung, Anthony K. H. and Han, Jiawei and Wang, Wei},
  editor = {Ng, Wee Keong and Li, Masaru Kitsuregawa and202007843@post au dk Jianzhong and Chang, Kuiyu},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3918},
  pages = {577--593},
  publisher = {Springer},
  doi = {10.1007/11731139_68}
}

@article{kangUNREXPLAINERCOUNTERFACTUALEXPLANATIONS2024,
  title = {{{UNR-EXPLAINER}}: {{COUNTERFACTUAL EXPLANATIONS FOR UNSUPERVISED NODE REPRESENTATION LEARN- ING MODELS}}},
  author = {Kang, Hyunju and Han, Geonhee and Park, Hogun},
  year = {2024},
  abstract = {Node representation learning, such as Graph Neural Networks (GNNs), has emerged as a pivotal method in machine learning. The demand for reliable explanation generation surges, yet unsupervised models remain underexplored in this regard. To bridge this gap, we introduce a method for generating counterfactual (CF) explanations in unsupervised node representation learning. We identify the most important subgraphs that cause a significant change in the k-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The k-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-k link prediction and clustering. Consequently, we introduce UNR-Explainer for generating expressive CF explanations for Unsupervised Node Representation learning methods based on a Monte Carlo Tree Search (MCTS). The proposed method demonstrates superior performance on diverse datasets for unsupervised GraphSAGE and DGI. Our codes are available at https://github.com/hjkng/unrexplainer.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/QCRGWKPZ/Kang et al. - 2024 - UNR-EXPLAINER COUNTERFACTUAL EXPLANATIONS FOR UNSUPERVISED NODE REPRESENTATION LEARN- ING MODELS.pdf}
}

@misc{karimiSurveyAlgorithmicRecourse2021,
  title = {A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects},
  shorttitle = {A Survey of Algorithmic Recourse},
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  year = {2021},
  month = mar,
  number = {arXiv:2010.04050},
  eprint = {2010.04050},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04050},
  urldate = {2025-02-07},
  abstract = {Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/9WHTWIGF/Karimi et al. - 2021 - A survey of algorithmic recourse definitions, formulations, solutions, and prospects.pdf}
}

@inproceedings{keaneIfOnlyWe2021,
  title = {If {{Only We Had Better Counterfactual Explanations}}: {{Five Key Deficits}} to {{Rectify}} in the {{Evaluation}} of {{Counterfactual XAI Techniques}}},
  shorttitle = {If {{Only We Had Better Counterfactual Explanations}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
  year = {2021},
  month = aug,
  pages = {4466--4474},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/609},
  urldate = {2025-02-14},
  abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI).  These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/UTDCD3V8/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations Five Key Deficits to Rectify in the Evaluation of.pdf}
}

@article{khanFaithfulCounterfactualVisual2024,
  title = {Faithful {{Counterfactual Visual Explanations}} ({{FCVE}})},
  author = {Khan, Bismillah and Tariq, Syed Ali and Zia, Tehseen and Ahsan, Muhammad and Windridge, David},
  year = {2024},
  month = jun,
  journal = {Knowledge-Based Systems},
  volume = {294},
  pages = {111668},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2024.111668},
  urldate = {2025-02-07},
  abstract = {Deep learning models in computer vision have made remarkable progress, but their lack of transparency and interpretability remains a challenge. The development of explainable AI can enhance the understanding and performance of these models. However, existing techniques often struggle to provide convincing explanations that non-experts easily understand, and they cannot accurately identify models' intrinsic decision-making processes. To address these challenges, we propose to develop a counterfactual explanation (CE) model that balances plausibility and faithfulness. This model generates easy-to-understand visual explanations by making minimum changes necessary in images without altering the pixel data. Instead, the proposed method identifies internal concepts and filters learned by models and leverages them to produce plausible counterfactual explanations. The provided explanations reflect the internal decision-making process of the model, thus ensuring faithfulness to the model.},
  keywords = {Counterfactual,Explainable AI,Visual explanation},
  file = {/home/jeppe/Zotero/storage/NVY8UZUP/S0950705124003034.html}
}

@article{kimExamplesAreNot,
  title = {Examples Are Not Enough, Learn to Criticize! {{Criticism}} for {{Interpretability}}},
  author = {Kim, Been},
  abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/IF6U4VQ7/Kim - Examples are not enough, learn to criticize! Criticism for Interpretability.pdf}
}

@article{liznerskiReimaginingAnomaliesWhat2024,
  title = {Reimagining {{Anomalies}}: {{What If Anomalies Were Normal}}?},
  author = {Liznerski, Philipp and Varshneya, Saurabh and Calikus, Ece and Fellenz, Sophie and Kloft, Marius},
  year = {2024},
  journal = {arXiv preprint arXiv:2402.14469},
  eprint = {2402.14469},
  archiveprefix = {arXiv}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, Stuart P.},
  year = {1982},
  journal = {IEEE Trans. Inf. Theory},
  volume = {28},
  number = {2},
  pages = {129--136},
  file = {/home/jeppe/Zotero/storage/BJDJD3GZ/Lloyd - 1982 - Least squares quantization in PCM.pdf;/home/jeppe/Zotero/storage/YDG4A9EQ/1056489.html}
}

@inproceedings{macqueenMethodsClassificationAnalysis1967,
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Statistics}}},
  author = {MacQueen, James},
  year = {1967},
  volume = {5},
  pages = {281--298},
  publisher = {University of California press},
  urldate = {2025-03-03},
  file = {/home/jeppe/Zotero/storage/WQCJCC9H/MacQueen - 1967 - Some methods for classification and analysis of multivariate observations.pdf}
}

@inproceedings{mothilalExplainingMachineLearning2020,
  title = {Explaining {{Machine Learning Classifiers}} through {{Diverse Counterfactual Explanations}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
  year = {2020},
  month = jan,
  eprint = {1905.07697},
  primaryclass = {cs},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  urldate = {2025-02-26},
  abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/PWSQFJVR/Mothilal et al. - 2020 - Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations.pdf;/home/jeppe/Zotero/storage/69N865QV/1905.html}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2025-03-07},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  file = {/home/jeppe/Zotero/storage/448FMAE6/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf}
}

@inproceedings{romashovBayConModelagnosticBayesian2022,
  title = {{{BayCon}}: {{Model-agnostic Bayesian Counterfactual Generator}}.},
  booktitle = {{{IJCAI}}},
  author = {Romashov, Piotr and Gjoreski, Martin and Sokol, Kacper and Martinez, Maria Vanina and Langheinrich, Marc},
  year = {2022},
  pages = {740--746}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2025-03-05},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jeppe/Zotero/storage/R9MPDCLL/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.pdf}
}

@article{spagnolCounterfactualExplanationsClustering2024,
  title = {Counterfactual {{Explanations}} for {{Clustering Models}}},
  author = {Spagnol, Aurora and Sokol, Kacper and Barbiero, Pietro and Langheinrich, Marc and Gjoreski, Martin},
  year = {2024},
  journal = {arXiv preprint arXiv:2409.12632},
  eprint = {2409.12632},
  archiveprefix = {arXiv}
}

@article{strutzDeterminingCountyLevelCounterfactuals2022,
  title = {Determining {{County-Level Counterfactuals}} for {{Evaluation}} of {{Population Health Interventions}}: {{A Novel Application}} of {{{\emph{K}}}} -{{Means Cluster Analysis}}},
  shorttitle = {Determining {{County-Level Counterfactuals}} for {{Evaluation}} of {{Population Health Interventions}}},
  author = {Strutz, Kelly L. and Luo, Zhehui and Raffo, Jennifer E. and Meghea, Cristian I. and Vander Meulen, Peggy and Roman, Lee Anne},
  year = {2022},
  month = sep,
  journal = {Public Health Reports{\textregistered}},
  volume = {137},
  number = {5},
  pages = {849--859},
  issn = {0033-3549, 1468-2877},
  doi = {10.1177/00333549211030507},
  urldate = {2025-02-07},
  abstract = {Objectives: Evaluating population health initiatives at the community level necessitates valid counterfactual communities, which includes having similar population composition, health care access, and health determinants. Estimating appropriate county counterfactuals is challenging in states with large intercounty variation. We describe an application of K-means cluster analysis for determining county-level counterfactuals in an evaluation of an intervention, a county perinatal system of care for Medicaid-insured pregnant women.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/RVYQHCHA/Strutz et al. - 2022 - Determining County-Level Counterfactuals for Evaluation of Population Health Interventions A Novel.pdf}
}

@inproceedings{vanlooverenInterpretableCounterfactualExplanations2021,
  title = {Interpretable {{Counterfactual Explanations Guided}} by {{Prototypes}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {Van Looveren, Arnaud and Klaise, Janis},
  editor = {Oliver, Nuria and {P{\'e}rez-Cruz}, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
  year = {2021},
  pages = {650--665},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-86520-7_40},
  abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the search for counterfactual instances and result in more interpretable explanations. We quantitatively evaluate interpretability of the generated counterfactuals to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). Additionally, we propose a principled approach to handle categorical variables and illustrate our method on the Adult (Census) dataset. Our method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for black box models.},
  isbn = {978-3-030-86520-7},
  langid = {english},
  keywords = {Counterfactual explanations,Interpretation,Transparency/Explainability},
  file = {/home/jeppe/Zotero/storage/DRLJ2A6X/Van Looveren and Klaise - 2021 - Interpretable Counterfactual Explanations Guided by Prototypes.pdf}
}

@misc{vardakasCounterfactualExplanationsKmeans2025,
  title = {Counterfactual {{Explanations}} for K-Means and {{Gaussian Clustering}}},
  author = {Vardakas, Georgios and Karra, Antonia and Pitoura, Evaggelia and Likas, Aristidis},
  year = {2025},
  month = jan,
  number = {arXiv:2501.10234},
  eprint = {2501.10234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10234},
  urldate = {2025-02-07},
  abstract = {Counterfactuals have been recognized as an effective approach to explain classifier decisions. Nevertheless, they have not yet been considered in the context of clustering. In this work, we propose the use of counterfactuals to explain clustering solutions. First, we present a general definition for counterfactuals for model-based clustering that includes plausibility and feasibility constraints. Then we consider the counterfactual generation problem for k-means and Gaussian clustering assuming Euclidean distance. Our approach takes as input the factual, the target cluster, a binary mask indicating actionable or immutable features and a plausibility factor specifying how far from the cluster boundary the counterfactual should be placed. In the k-means clustering case, analytical mathematical formulas are presented for computing the optimal solution, while in the Gaussian clustering case (assuming full, diagonal, or spherical covariances) our method requires the numerical solution of a nonlinear equation with a single parameter only. We demonstrate the advantages of our approach through illustrative examples and quantitative experimental comparisons.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeppe/Zotero/storage/6J4NRSPK/Vardakas et al. - 2025 - Counterfactual Explanations for k-means and Gaussian Clustering.pdf}
}

@article{wachterCounterfactualExplanationsOpening2017,
  title = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}: {{Automated Decisions}} and the {{GDPR}}},
  shorttitle = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2017},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3063289},
  urldate = {2025-02-07},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/HMFFLS7N/Wachter et al. - 2017 - Counterfactual Explanations Without Opening the Black Box Automated Decisions and the GDPR.pdf}
}

@article{wardHierarchicalGroupingOptimize1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward, Joe H.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244}
}
