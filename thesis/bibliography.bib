@misc{amirbeckFastIterativeShrinkageThresholding,
  title = {A {{Fast Iterative Shrinkage-Thresholding Algorithm}} for {{Linear Inverse Problems}}},
  author = {Amir Beck, Marc Teboulle},
  doi = {10.1137/080716542},
  urldate = {2025-03-12},
  howpublished = {https://epubs.siam.org/doi/epdf/10.1137/080716542},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/CGS5WRZX/080716542.html;/home/jeppe/Zotero/storage/P8SC7ZUC/080716542.html}
}

@article{angelovExplainableArtificialIntelligence2021,
  title = {Explainable Artificial Intelligence: An Analytical Review},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Angelov, Plamen P. and Soares, Eduardo A. and Jiang, Richard and Arnold, Nicholas I. and Atkinson, Peter M.},
  year = {2021},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {11},
  number = {5},
  pages = {e1424},
  issn = {1942-4795},
  doi = {10.1002/widm.1424},
  urldate = {2025-03-05},
  abstract = {This paper provides a brief analytical review of the current state-of-the-art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested. This article is categorized under: Technologies {$>$} Artificial Intelligence Fundamental Concepts of Data and Knowledge {$>$} Explainable AI},
  langid = {english},
  keywords = {black-box models,deep learning,explainable AI,machine learning,prototype-based models,surrogate models},
  file = {/home/jeppe/Zotero/storage/YX98RMPV/Angelov et al. - 2021 - Explainable artificial intelligence an analytical review.pdf;/home/jeppe/Zotero/storage/3U5AZGCG/widm.html}
}

@inproceedings{angiulliCounterfactualsExplanationsOutliers2023,
  title = {Counterfactuals {{Explanations}} for {{Outliers}} via {{Subspaces Density Contrastive Loss}}},
  booktitle = {International {{Conference}} on {{Discovery Science}}},
  author = {Angiulli, Fabrizio and Fassetti, Fabio and Nistic{\'o}, Simona and Palopoli, Luigi},
  year = {2023},
  pages = {159--173},
  publisher = {Springer}
}

@article{arthurKmeansAdvantagesCareful,
  title = {K-Means++: {{The Advantages}} of {{Careful Seeding}}},
  author = {Arthur, David and Vassilvitskii, Sergei},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is O(log k)-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/YFK27HE5/Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf}
}

@article{bentleyMultidimensionalBinarySearch1975,
  title = {Multidimensional Binary Search Trees Used for Associative Searching},
  author = {Bentley, Jon Louis},
  year = {1975},
  month = sep,
  journal = {Communications of the ACM},
  volume = {18},
  number = {9},
  pages = {509--517},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/361002.361007},
  urldate = {2025-03-05},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/DZWMR5DJ/Bentley - 1975 - Multidimensional binary search trees used for associative searching.pdf}
}

@inproceedings{bhattExplainableMachineLearning2020,
  title = {Explainable Machine Learning in Deployment},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} M. F. and Eckersley, Peter},
  year = {2020},
  month = jan,
  pages = {648--657},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3351095.3375624},
  urldate = {2025-03-03},
  abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/XQ4XPKLT/Bhatt et al. - 2020 - Explainable machine learning in deployment.pdf}
}

@inproceedings{breunigLOFIdentifyingDensitybased2000,
  title = {{{LOF}}: Identifying Density-Based Local Outliers},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  year = {2000},
  pages = {93--104}
}

@inproceedings{byrneCounterfactualsExplainableArtificial2019,
  title = {Counterfactuals in {{Explainable Artificial Intelligence}} ({{XAI}}): {{Evidence}} from {{Human Reasoning}}},
  shorttitle = {Counterfactuals in {{Explainable Artificial Intelligence}} ({{XAI}})},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Byrne, Ruth M. J.},
  year = {2019},
  month = aug,
  pages = {6276--6282},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macao, China},
  doi = {10.24963/ijcai.2019/876},
  urldate = {2025-03-07},
  abstract = {Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI).  Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/6B5QKH2E/Byrne - 2019 - Counterfactuals in Explainable Artificial Intelligence (XAI) Evidence from Human Reasoning.pdf}
}

@inproceedings{chawlaKmeansUnifiedApproach2013,
  title = {K-Means--: {{A Unified Approach}} to {{Clustering}} and {{Outlier Detection}}},
  booktitle = {Proceedings of the 13th {{SIAM International Conference}} on {{Data Mining}}, {{May}} 2-4, 2013. {{Austin}}, {{Texas}}, {{USA}}},
  author = {Chawla, Sanjay and Gionis, Aristides},
  year = {2013},
  pages = {189--197},
  publisher = {SIAM},
  doi = {10.1137/1.9781611972832.21}
}

@article{dasguptaExplainableKMeansKMedians2020,
  title = {Explainable K-{{Means}} and k-{{Medians Clustering}}},
  author = {Dasgupta, Sanjoy and Frost, Nave and Moshkovitz, Michal and Rashtchian, Cyrus},
  year = {2020},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/IX98EYLP/Dasgupta et al. - Explainable k-Means and k-Medians Clustering.pdf}
}

@inproceedings{dasilvaUsingKassociatedOptimal2022,
  title = {Using the {{K-associated Optimal Graph}} to {{Provide Counterfactual Explanations}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Fuzzy Systems}} ({{FUZZ-IEEE}})},
  author = {{da Silva}, Ariel Tadeu and Bertini, Jo{\~a}o Roberto},
  year = {2022},
  month = jul,
  pages = {1--8},
  issn = {1558-4739},
  doi = {10.1109/FUZZ-IEEE55066.2022.9882751},
  urldate = {2025-02-07},
  abstract = {Only recently have data mining results been thought to aid human interpretability. Explanations are useful to understand the reasons why (or why not) the model has (or hasn't) achieved a given decision. Counterfactual explanations aim to explain why not the model yield an expected decision. A counterfactual explanation is usually done by a post-hoc algorithm which often requires access to training data or model details. Also, the majority of such algorithms do not generate robust explanations, once they assume the model is noise-free and will not be updated over time. This paper proposes a model-agnostic, training data-independent algorithm to provide robust counterfactual explanations. The proposed method generates data samples around the instance to be explained and builds a K-Associated Optimal Graph (KAOG) with those data. KAOG allows measuring how intertwined the data examples are regarding their classes. This way, the explanation method can search for an example that relies on a noise-free area in the attribute space, granting trust to the explanation. Experiment results on counterfactual feasibility and distance from query data show the effectiveness of the proposed algorithm when compared to ten state-of-the-art methods on three data sets.},
  keywords = {Data mining,Data models,Fuzzy systems,Robustness,Training,Training data},
  file = {/home/jeppe/Zotero/storage/FJY4S65Q/da Silva and Bertini - 2022 - Using the K-associated Optimal Graph to Provide Counterfactual Explanations.pdf;/home/jeppe/Zotero/storage/L5JFQK2Q/9882751.html}
}

@misc{ellisAlgorithmAgnosticExplainabilityUnsupervised2021,
  title = {Algorithm-{{Agnostic Explainability}} for {{Unsupervised Clustering}}},
  author = {Ellis, Charles A. and Sendi, Mohammad S. E. and Geenjaar, Eloy P. T. and Plis, Sergey M. and Miller, Robyn L. and Calhoun, Vince D.},
  year = {2021},
  month = aug,
  number = {arXiv:2105.08053},
  eprint = {2105.08053},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.08053},
  urldate = {2025-03-10},
  abstract = {Supervised machine learning explainability has developed rapidly in recent years. However, clustering explainability has lagged behind. Here, we demonstrate the first adaptation of model-agnostic explainability methods to explain unsupervised clustering. We present two novel "algorithm-agnostic" explainability methods -- global permutation percent change (G2PC) and local perturbation percent change (L2PC) -- that identify feature importance globally to a clustering algorithm and locally to the clustering of individual samples. The methods are (1) easy to implement and (2) broadly applicable across clustering algorithms, which could make them highly impactful. We demonstrate the utility of the methods for explaining five popular clustering methods on low-dimensional synthetic datasets and on high-dimensional functional network connectivity data extracted from a resting-state functional magnetic resonance imaging dataset of 151 individuals with schizophrenia and 160 controls. Our results are consistent with existing literature while also shedding new light on how changes in brain connectivity may lead to schizophrenia symptoms. We further compare the explanations from our methods to an interpretable classifier and find them to be highly similar. Our proposed methods robustly explain multiple clustering algorithms and could facilitate new insights into many applications. We hope this study will greatly accelerate the development of the field of clustering explainability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeppe/Zotero/storage/D5RWWLHT/Ellis et al. - 2021 - Algorithm-Agnostic Explainability for Unsupervised Clustering.pdf}
}

@inproceedings{esterDensityBasedAlgorithmDiscovering1996,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} ({{KDD-96}}), {{Portland}}, {{Oregon}}, {{USA}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  editor = {Simoudis, Evangelos and Han, Jiawei and Fayyad, Usama M.},
  year = {1996},
  pages = {226--231},
  publisher = {AAAI Press}
}

@article{estevaDermatologistlevelClassificationSkin2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  year = {2017},
  month = feb,
  journal = {Nature},
  volume = {542},
  number = {7639},
  pages = {115--118},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature21056},
  urldate = {2025-03-03},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/ZLWAMJ3V/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf}
}

@article{EU_AI_Act_Article_86,
  title = {Article 86: {{Right}} to Explanation of Individual Decision-Making},
  journal = {EU Artificial Intelligence Act}
}

@misc{frazierTutorialBayesianOptimization2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02811},
  eprint = {1807.02811},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.02811},
  urldate = {2025-02-24},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/ZMH6FHZK/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf}
}

@article{guidottiCounterfactualExplanationsHow2024,
  title = {Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking},
  author = {Guidotti, Riccardo},
  year = {2024},
  journal = {Data Mining and Knowledge Discovery},
  volume = {38},
  number = {5},
  pages = {2770--2824},
  publisher = {Springer}
}

@inproceedings{hanAchievingCounterfactualFairness2023,
  title = {Achieving Counterfactual Fairness for Anomaly Detection},
  booktitle = {Pacific-{{Asia Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Han, Xiao and Zhang, Lu and Wu, Yongkai and Yuan, Shuhan},
  year = {2023},
  pages = {55--66},
  publisher = {Springer}
}

@inproceedings{jinRankingOutliersUsing2006,
  title = {Ranking {{Outliers Using Symmetric Neighborhood Relationship}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}, 10th {{Pacific-Asia Conference}}, {{PAKDD}} 2006, {{Singapore}}, {{April}} 9-12, 2006, {{Proceedings}}},
  author = {Jin, Wen and Tung, Anthony K. H. and Han, Jiawei and Wang, Wei},
  editor = {Ng, Wee Keong and Li, Masaru Kitsuregawa and202007843@post au dk Jianzhong and Chang, Kuiyu},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3918},
  pages = {577--593},
  publisher = {Springer},
  doi = {10.1007/11731139_68}
}

@article{kangUNREXPLAINERCOUNTERFACTUALEXPLANATIONS2024,
  title = {{{UNR-EXPLAINER}}: {{COUNTERFACTUAL EXPLANATIONS FOR UNSUPERVISED NODE REPRESENTATION LEARN- ING MODELS}}},
  author = {Kang, Hyunju and Han, Geonhee and Park, Hogun},
  year = {2024},
  abstract = {Node representation learning, such as Graph Neural Networks (GNNs), has emerged as a pivotal method in machine learning. The demand for reliable explanation generation surges, yet unsupervised models remain underexplored in this regard. To bridge this gap, we introduce a method for generating counterfactual (CF) explanations in unsupervised node representation learning. We identify the most important subgraphs that cause a significant change in the k-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The k-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-k link prediction and clustering. Consequently, we introduce UNR-Explainer for generating expressive CF explanations for Unsupervised Node Representation learning methods based on a Monte Carlo Tree Search (MCTS). The proposed method demonstrates superior performance on diverse datasets for unsupervised GraphSAGE and DGI. Our codes are available at https://github.com/hjkng/unrexplainer.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/QCRGWKPZ/Kang et al. - 2024 - UNR-EXPLAINER COUNTERFACTUAL EXPLANATIONS FOR UNSUPERVISED NODE REPRESENTATION LEARN- ING MODELS.pdf}
}

@misc{karimiSurveyAlgorithmicRecourse2021,
  title = {A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects},
  shorttitle = {A Survey of Algorithmic Recourse},
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  year = {2021},
  month = mar,
  number = {arXiv:2010.04050},
  eprint = {2010.04050},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04050},
  urldate = {2025-02-07},
  abstract = {Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/9WHTWIGF/Karimi et al. - 2021 - A survey of algorithmic recourse definitions, formulations, solutions, and prospects.pdf}
}

@inproceedings{keaneIfOnlyWe2021,
  title = {If {{Only We Had Better Counterfactual Explanations}}: {{Five Key Deficits}} to {{Rectify}} in the {{Evaluation}} of {{Counterfactual XAI Techniques}}},
  shorttitle = {If {{Only We Had Better Counterfactual Explanations}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
  year = {2021},
  month = aug,
  pages = {4466--4474},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/609},
  urldate = {2025-02-14},
  abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI).  These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/UTDCD3V8/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations Five Key Deficits to Rectify in the Evaluation of.pdf}
}

@article{khanFaithfulCounterfactualVisual2024,
  title = {Faithful {{Counterfactual Visual Explanations}} ({{FCVE}})},
  author = {Khan, Bismillah and Tariq, Syed Ali and Zia, Tehseen and Ahsan, Muhammad and Windridge, David},
  year = {2024},
  month = jun,
  journal = {Knowledge-Based Systems},
  volume = {294},
  pages = {111668},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2024.111668},
  urldate = {2025-02-07},
  abstract = {Deep learning models in computer vision have made remarkable progress, but their lack of transparency and interpretability remains a challenge. The development of explainable AI can enhance the understanding and performance of these models. However, existing techniques often struggle to provide convincing explanations that non-experts easily understand, and they cannot accurately identify models' intrinsic decision-making processes. To address these challenges, we propose to develop a counterfactual explanation (CE) model that balances plausibility and faithfulness. This model generates easy-to-understand visual explanations by making minimum changes necessary in images without altering the pixel data. Instead, the proposed method identifies internal concepts and filters learned by models and leverages them to produce plausible counterfactual explanations. The provided explanations reflect the internal decision-making process of the model, thus ensuring faithfulness to the model.},
  keywords = {Counterfactual,Explainable AI,Visual explanation},
  file = {/home/jeppe/Zotero/storage/NVY8UZUP/S0950705124003034.html}
}

@article{kimExamplesAreNot,
  title = {Examples Are Not Enough, Learn to Criticize! {{Criticism}} for {{Interpretability}}},
  author = {Kim, Been},
  abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/IF6U4VQ7/Kim - Examples are not enough, learn to criticize! Criticism for Interpretability.pdf}
}

@article{kuleszaDeterminantalPointProcesses2012,
  title = {Determinantal Point Processes for Machine Learning},
  author = {Kulesza, Alex and Taskar, Ben},
  year = {2012},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {5},
  number = {2-3},
  eprint = {1207.6083},
  primaryclass = {stat},
  pages = {1--6},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000044},
  urldate = {2025-03-12},
  abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/X6KN8Z7C/Kulesza and Taskar - 2012 - Determinantal point processes for machine learning.pdf}
}

@article{liznerskiReimaginingAnomaliesWhat2024,
  title = {Reimagining {{Anomalies}}: {{What If Anomalies Were Normal}}?},
  author = {Liznerski, Philipp and Varshneya, Saurabh and Calikus, Ece and Fellenz, Sophie and Kloft, Marius},
  year = {2024},
  journal = {arXiv preprint arXiv:2402.14469},
  eprint = {2402.14469},
  archiveprefix = {arXiv}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, Stuart P.},
  year = {1982},
  journal = {IEEE Trans. Inf. Theory},
  volume = {28},
  number = {2},
  pages = {129--136},
  file = {/home/jeppe/Zotero/storage/BJDJD3GZ/Lloyd - 1982 - Least squares quantization in PCM.pdf;/home/jeppe/Zotero/storage/YDG4A9EQ/1056489.html}
}

@inproceedings{macqueenMethodsClassificationAnalysis1967,
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Statistics}}},
  author = {MacQueen, James},
  year = {1967},
  volume = {5},
  pages = {281--298},
  publisher = {University of California press},
  urldate = {2025-03-03},
  file = {/home/jeppe/Zotero/storage/WQCJCC9H/MacQueen - 1967 - Some methods for classification and analysis of multivariate observations.pdf}
}

@misc{maxMaxidlMMDcriticGitHub2020,
  title = {Maxidl/{{MMD-critic}} {$\cdot$} {{GitHub}}},
  author = {Max, Idahl},
  year = {2020},
  month = nov,
  urldate = {2025-03-10},
  howpublished = {https://github.com/maxidl/MMD-critic},
  file = {/home/jeppe/Zotero/storage/VRWYWN7W/MMD-critic.html}
}

@inproceedings{mothilalExplainingMachineLearning2020,
  title = {Explaining {{Machine Learning Classifiers}} through {{Diverse Counterfactual Explanations}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
  year = {2020},
  month = jan,
  eprint = {1905.07697},
  primaryclass = {cs},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  urldate = {2025-02-26},
  abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jeppe/Zotero/storage/PWSQFJVR/Mothilal et al. - 2020 - Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations.pdf;/home/jeppe/Zotero/storage/69N865QV/1905.html}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2025-03-07},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  file = {/home/jeppe/Zotero/storage/448FMAE6/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf}
}

@inproceedings{romashovBayConModelagnosticBayesian2022,
  title = {{{BayCon}}: {{Model-agnostic Bayesian Counterfactual Generator}}.},
  booktitle = {{{IJCAI}}},
  author = {Romashov, Piotr and Gjoreski, Martin and Sokol, Kacper and Martinez, Maria Vanina and Langheinrich, Marc},
  year = {2022},
  pages = {740--746}
}

@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2025-03-05},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jeppe/Zotero/storage/R9MPDCLL/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.pdf}
}

@article{spagnolCounterfactualExplanationsClustering2024,
  title = {Counterfactual {{Explanations}} for {{Clustering Models}}},
  author = {Spagnol, Aurora and Sokol, Kacper and Barbiero, Pietro and Langheinrich, Marc and Gjoreski, Martin},
  year = {2024},
  journal = {arXiv preprint arXiv:2409.12632},
  eprint = {2409.12632},
  archiveprefix = {arXiv}
}

@article{strutzDeterminingCountyLevelCounterfactuals2022,
  title = {Determining {{County-Level Counterfactuals}} for {{Evaluation}} of {{Population Health Interventions}}: {{A Novel Application}} of {{{\emph{K}}}} -{{Means Cluster Analysis}}},
  shorttitle = {Determining {{County-Level Counterfactuals}} for {{Evaluation}} of {{Population Health Interventions}}},
  author = {Strutz, Kelly L. and Luo, Zhehui and Raffo, Jennifer E. and Meghea, Cristian I. and Vander Meulen, Peggy and Roman, Lee Anne},
  year = {2022},
  month = sep,
  journal = {Public Health Reports{\textregistered}},
  volume = {137},
  number = {5},
  pages = {849--859},
  issn = {0033-3549, 1468-2877},
  doi = {10.1177/00333549211030507},
  urldate = {2025-02-07},
  abstract = {Objectives: Evaluating population health initiatives at the community level necessitates valid counterfactual communities, which includes having similar population composition, health care access, and health determinants. Estimating appropriate county counterfactuals is challenging in states with large intercounty variation. We describe an application of K-means cluster analysis for determining county-level counterfactuals in an evaluation of an intervention, a county perinatal system of care for Medicaid-insured pregnant women.},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/RVYQHCHA/Strutz et al. - 2022 - Determining County-Level Counterfactuals for Evaluation of Population Health Interventions A Novel.pdf}
}

@inproceedings{vanlooverenInterpretableCounterfactualExplanations2021,
  title = {Interpretable {{Counterfactual Explanations Guided}} by {{Prototypes}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {Van Looveren, Arnaud and Klaise, Janis},
  editor = {Oliver, Nuria and {P{\'e}rez-Cruz}, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
  year = {2021},
  pages = {650--665},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-86520-7_40},
  abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the search for counterfactual instances and result in more interpretable explanations. We quantitatively evaluate interpretability of the generated counterfactuals to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). Additionally, we propose a principled approach to handle categorical variables and illustrate our method on the Adult (Census) dataset. Our method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for black box models.},
  isbn = {978-3-030-86520-7},
  langid = {english},
  keywords = {Counterfactual explanations,Interpretation,Transparency/Explainability},
  file = {/home/jeppe/Zotero/storage/DRLJ2A6X/Van Looveren and Klaise - 2021 - Interpretable Counterfactual Explanations Guided by Prototypes.pdf}
}

@misc{vardakasCounterfactualExplanationsKmeans2025,
  title = {Counterfactual {{Explanations}} for K-Means and {{Gaussian Clustering}}},
  author = {Vardakas, Georgios and Karra, Antonia and Pitoura, Evaggelia and Likas, Aristidis},
  year = {2025},
  month = jan,
  number = {arXiv:2501.10234},
  eprint = {2501.10234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10234},
  urldate = {2025-02-07},
  abstract = {Counterfactuals have been recognized as an effective approach to explain classifier decisions. Nevertheless, they have not yet been considered in the context of clustering. In this work, we propose the use of counterfactuals to explain clustering solutions. First, we present a general definition for counterfactuals for model-based clustering that includes plausibility and feasibility constraints. Then we consider the counterfactual generation problem for k-means and Gaussian clustering assuming Euclidean distance. Our approach takes as input the factual, the target cluster, a binary mask indicating actionable or immutable features and a plausibility factor specifying how far from the cluster boundary the counterfactual should be placed. In the k-means clustering case, analytical mathematical formulas are presented for computing the optimal solution, while in the Gaussian clustering case (assuming full, diagonal, or spherical covariances) our method requires the numerical solution of a nonlinear equation with a single parameter only. We demonstrate the advantages of our approach through illustrative examples and quantitative experimental comparisons.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jeppe/Zotero/storage/6J4NRSPK/Vardakas et al. - 2025 - Counterfactual Explanations for k-means and Gaussian Clustering.pdf}
}

@article{wachterCounterfactualExplanationsOpening2017,
  title = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}: {{Automated Decisions}} and the {{GDPR}}},
  shorttitle = {Counterfactual {{Explanations Without Opening}} the {{Black Box}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2017},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3063289},
  urldate = {2025-02-07},
  langid = {english},
  file = {/home/jeppe/Zotero/storage/HMFFLS7N/Wachter et al. - 2017 - Counterfactual Explanations Without Opening the Black Box Automated Decisions and the GDPR.pdf}
}

@article{wardHierarchicalGroupingOptimize1963,
  title = {Hierarchical {{Grouping}} to {{Optimize}} an {{Objective Function}}},
  author = {Ward, Joe H.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244}
}

@misc{ScikitlearnMachineLearning,
  title = {Scikit-Learn: Machine Learning in {{Python}} --- Scikit-Learn 1.6.1 Documentation},
  urldate = {2025-03-19},
  howpublished = {https://scikit-learn.org/stable/},
  file = {C:\Users\kmpos\Zotero\storage\G53K9BYH\stable.html}
}

@article{gawiejnowiczKnapsackProblemsPositiondependent2023,
  title = {Knapsack Problems with Position-Dependent Item Weights or Profits},
  author = {Gawiejnowicz, Stanis{\l}aw and Halman, Nir and Kellerer, Hans},
  year = {2023},
  month = jul,
  journal = {Annals of Operations Research},
  volume = {326},
  number = {1},
  pages = {137--156},
  issn = {1572-9338},
  doi = {10.1007/s10479-023-05265-x},
  urldate = {2025-03-21},
  abstract = {We consider three new knapsack problems with variable weights or profits of items, where the weight or profit of an item depends on the position of the item in the sequence of items packed in the knapsack. We show how to solve the problems exactly using dynamic programming algorithms with pseudo-polynomial running times and propose fully polynomial-time approximation schemes for their approximate solution.},
  langid = {english},
  keywords = {Approximation schemes,Dynamic programming,Knapsack problem},
  file = {C:\Users\kmpos\Zotero\storage\Q96M7MH3\Gawiejnowicz m.fl. - 2023 - Knapsack problems with position-dependent item weights or profits.pdf}
}

@article{nemhauserBestAlgorithmsApproximating1978,
  title = {Best {{Algorithms}} for {{Approximating}} the {{Maximum}} of a {{Submodular Set Function}}},
  author = {Nemhauser, G. L. and Wolsey, L. A.},
  year = {1978},
  journal = {Mathematics of Operations Research},
  volume = {3},
  number = {3},
  eprint = {3689488},
  eprinttype = {jstor},
  pages = {177--188},
  publisher = {INFORMS},
  issn = {0364-765X},
  urldate = {2025-03-24},
  abstract = {A real-valued function z whose domain is all of the subsets of N = \{1,..., n\} is said to be submodular if {$<$}tex-math{$>\$$}z(S)+z(T){\textbackslash}geq z(S{\textbackslash}cup T)+z(S{\textbackslash}cap T),{\textbackslash}forall S,T{\textbackslash}subseteq N\${$<$}/tex-math{$>$}, and nondecreasing if {$<$}tex-math{$>\$$}z(S){\textbackslash}leq z(T),{\textbackslash}forall S{\textbackslash}subset T{\textbackslash}subseteq N\${$<$}/tex-math{$>$}. We consider the problem {$<$}tex-math{$>\$$}\{{\textbackslash}rm max\}\_\{S{\textbackslash}subset N\}{\textbackslash} {\textbackslash}\{z(S){\textbackslash}colon {\textbar}S{\textbar}{\textbackslash}geq K\${$<$}/tex-math{$>$}, z submodular and nondecreasing, z({$\varphi$}) = 0\}. Many combinatorial optimization problems can be posed in this framework. For example, a well-known location problem and the maximization of certain boolean polynomials are in this class. We present a family of algorithms that involve the partial enumeration of all sets of cardinality q and then a greedy selection of the remaining elements, q = 0,..., K - l. For fixed K, the qth member of this family requires {$<$}tex-math{$>\$$}O(n{\textasciicircum}\{q+1\})\${$<$}/tex-math{$>$} computations and is guaranteed to achieve at least {$<$}tex-math{$>\$$}[1-({\textbackslash}frac\{K-q\}\{K\})({\textbackslash}frac\{K-q-1\}\{K-q\}){\textasciicircum}\{K-q\}]{\textbackslash}times 100\${$<$}/tex-math{$>$} percent of the optimum value. Our main result is that this is the best performance guarantee that can be obtained by any algorithm whose number of computations does not exceed {$<$}tex-math{$>\$$}O(n{\textasciicircum}\{q+1\})\${$<$}/tex-math{$>$}.},
  file = {C:\Users\kmpos\Zotero\storage\TCZLW4YA\Nemhauser and Wolsey - 1978 - Best Algorithms for Approximating the Maximum of a Submodular Set Function.pdf}
}

@misc{sviridenkoOptimalApproximationSubmodular2014,
  title = {Optimal Approximation for Submodular and Supermodular Optimization with Bounded Curvature},
  author = {Sviridenko, Maxim and Vondr{\'a}k, Jan and Ward, Justin},
  year = {2014},
  month = dec,
  number = {arXiv:1311.4728},
  eprint = {1311.4728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.4728},
  urldate = {2025-03-24},
  abstract = {We design new approximation algorithms for the problems of optimizing submodular and supermodular functions subject to a single matroid constraint. Specifically, we consider the case in which we wish to maximize a nondecreasing submodular function or minimize a nonincreasing supermodular function in the setting of bounded total curvature c. In the case of submodular maximization with curvature c, we obtain a (1 - c/e)-approximation --- the first improvement over the greedy (1 - e-c)/c-approximation of Conforti and Cornuejols from 1984, which holds for a cardinality constraint, as well as recent approaches that hold for an arbitrary matroid constraint.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {C:\Users\kmpos\Zotero\storage\ZGZGYT7C\Sviridenko m.fl. - 2014 - Optimal approximation for submodular and supermodular optimization with bounded curvature.pdf}
}

@inproceedings{cravenExtractingTreeStructuredRepresentations1995,
  title = {Extracting {{Tree-Structured Representations}} of {{Trained Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Craven, Mark and Shavlik, Jude},
  year = {1995},
  volume = {8},
  publisher = {MIT Press},
  urldate = {2025-04-02},
  abstract = {A  significant  limitation  of  neural  networks  is  that  the  represen(cid:173) tations  they  learn  are  usually  incomprehensible  to  humans.  We  present a  novel algorithm , TREPAN,  for extracting comprehensible,  symbolic representations from  trained neural  networks.  Our algo(cid:173) rithm uses  queries to induce a  decision tree that approximates the  concept  represented by  a  given  network.  Our experiments demon(cid:173) strate that TREPAN is  able to produce decision trees that maintain  a high level of fidelity to their respective networks while being com(cid:173) prehensible  and  accurate.  Unlike  previous  work  in  this  area,  our  algorithm is  general in its applicability and scales well  to large net(cid:173) works  and problems with  high-dimensional input spaces.},
  file = {/home/donut/Zotero/storage/QKIDF8T6/Craven and Shavlik - 1995 - Extracting Tree-Structured Representations of Trained Networks.pdf}
}
