#import "../lib.typ": *

= Explainable AI<sec:XAI>
//TODO: chapter intro? explain goal of chapter, focus of thesis etc.?
//maybe outline some different types of explanations? feature importance, gradients, etc?

The field of machine learning (ML) has made great strides in recent years, resulting in much better classification accuracy and more complex models with potentially millions of parameters. Given the impressive performance of these models, the industry has been quick to adopt them into their artificial intelligence (AI) systems in production.

However, these models are often used as black boxes: a tool which, when given an input, produces an output with no explanation or justification of how or why the model returned the given result. Furthermore, these advanced models have been applied in critical fields like cancer diagnosis @estevaDermatologistlevelClassificationSkin2017 or bank loan approval @bhattExplainableMachineLearning2020, both of which require trust in the model in order to satisfy stakeholders. For example, patients confronted with an AI-driven assessment of potentially malignant skin lesions would reasonably expect either an explanation of the model's conclusion or validation from a medical professional. This problem has given rise to the field of explainable AI (XAI), which is concerned with "opening the black box" by developing methods that can explain the decisions made by these models. Additionally, XAI can also help combat bias and discrimination caused by AI systems, by investigating if the model is making decisions based primarily on one or more sensitive attributes like age, gender or race. 

The problem of black-box AI systems has also recently led to legislation by the EU as part of the AI act @EU_AI_Act_Article_86. In the legislation, article 86 ensures the "right to explanation" for individuals in the EU, when affected by the decision of a high-risk AI system, defined as systems acting in the area of biometrics, education, employment, law enforcement, etc. Here, the right to explanation means the affected subject can obtain an explanation of how AI was used in the decision, and the main factors contributing to the decision. When the article goes into effect in 2026, organizations deploying AI systems will likely need XAI methods to comply with the new regulations.

Finally, XAI is also interesting from a developer's perspective. By understanding how a model works, designers can better understand the strengths and weaknesses of their models, and potentially improve them. For example, one can use gradient-based explanation methods like Grad-CAM @selvarajuGradCAMVisualExplanations2020 for convolutional neural networks in order to understand which areas of an image contributed most to a classification (@fig:grad_cam_example). In combination with domain knowledge, a developer can quickly use this to diagnose issues with a model, and thereby improve it. 
#figure(
    image("../assets/gradcam_example.png", width: 75%),
    caption: [Example of an explanation generated by Grad-CAM @selvarajuGradCAMVisualExplanations2020.],
)<fig:grad_cam_example>

== Properties of explanations <sec:properties_of_explanations>
There exists a large number of ways to explain ML models. These explanations can be broadly categorized using the following categories, based loosely on the taxonomy from @angelovExplainableArtificialIntelligence2021.

*Local vs. global explanations*. When seeking an explanation, one can either try to explain the entire model globally, or one can explain the model locally given a single input. As an example, the prediction given by Grad-CAM in @fig:grad_cam_example is a local explanation of an instance, where it highlights which parts of the image have had the greatest effect on the prediction. Alternatively, the well-known LIME feature-attribution method @ribeiroWhyShouldTrust2016 provides primarily local explanations, but also offers global perspective by picking a limited number of instances along with their explanations while minimizing redundancy. // Better example of some global method?

*Model-specific vs. model-agnostic explanations*. An explanation method may place certain requirements on the model being explained, meaning it does not treat it as a true "black box". For example, gradient-based methods like Grad-CAM require a differentiable model. Other methods like LIME place no requirements on the model to be explained, only needing access to a function $f$ which returns a prediction when given an instance. Although model-agnostic methods are desirable due to their flexibility, model-specific methods are able to exploit the properties of certain models to achieve improvements in performance. 

*Post-hoc vs. intrinsic explanations*. A post-hoc explanation method generates explanations for an existing, trained model, whereas intrinsic methods instead produce a model based on a combined objective of achieving high accuracy, while also ensuring the final model is explainable. In this context, there is often an accuracy/explainability trade-off, since explainability often amounts to having a model that is simple enough for humans to understand. In literature, such models are also described as transparent, or white-box methods (as opposed to black-box). As an example, the Iterative Mistake Minimization method presented in @dasguptaExplainableKMeansKMedians2020 creates a global clustering explanation by approximating reference clusters using threshold trees. The trade-off then arises because the tree should be small for explainability, whereas a larger tree is able to better approximate the reference clusters.

*Exogenous vs. endogenous explanations*. In the context of counterfactual explanations, which we introduce in detail in @sec:counterfactual_explanations, an explanation is a single point from the same domain as the instance being explained. In such cases, the point may either be a point picked from the dataset (endogenous), or it can be a novel point generated by the explanation method (exogenous). Although most methods generate exogenous explanations, endogenous methods offer a natural guarantee that the explanations are realistic or plausible in relation to the existing data.

== Counterfactual explanations <sec:counterfactual_explanations>
The focus of this thesis is placed primarily on counterfactual explanations. In this section, we introduce the intuition behind them, along with basic notation used throughout the thesis. 

Counterfactual explanations answer the question of "what should be done to change this outcome?". Thus, they are a form of local explanation, always explaining a single instance $x$. The typical example used in literature is based on binary classification, where a model $f$ outputs $1$ if a given instance representing a loan application was accepted for a loan, and outputs $0$ if it was denied (see @fig:CF_example). In this case, given some instance $x in RR^d$ where $f(x)=0$ a counterfactual explanation is a different instance $x' in RR^d$ s.t. $f(x')=1$ with the additional requirement that $x'$ should be close to $x$. Thus, the counterfactual would be a different, but similar, application which was accepted for a loan. This type of explanation offers an intuitive way for users to understand what should be done in order to change a prediction. In the example above, the counterfactual application $x'$ might have a higher income than the original application $x$, which helps in identifying cause-effect relations in the underlying model. This type of explanation is intuitive to people according to the cognitive psychology literature @byrneCounterfactualsExplainableArtificial2019. Counterfactuals can also serve as a useful explanation for practitioners, allowing them to understand which features might be important for a model to output a certain result.

For example, there have been real-world cases where a model turned out to be biased against certain minority groups, like the famous COMPAS system which was used to predict recidivism in criminal defendants @angwinMachineBias2022. This system turned out to be heavily biased against black people, predicting much higher risks of recidivism than it did for white defendants. Counterfactual explanations can help identify these kinds of biases in a model. For example, if many counterfactuals suggest changing a sensitive attribute, there is a high risk that the model is biased.

#figure(
    image("../assets/CF_example.png", width: 50%),
    caption: [Simple visual example of a counterfactual explanation. $x$ is the original instance, and $x'$ is a counterfactual. The decision boundary is marked by the black line.],
)<fig:CF_example>

When evaluating such explanations, there is no single criterion for what makes a good counterfactual. However, based on the literature, we present some properties that one might consider when designing and evaluating counterfactual explainers @guidottiCounterfactualExplanationsHow2024. First, and most importantly, a counterfactual should be valid, meaning it should lead to a different prediction than the original i.e. $f(x) eq.not f(x')$. Secondly, the counterfactual should present a _minimal_ change to the original instance. This can be represented using a simple similarity measure between the two points $x "and" x'$, but many methods also take into account the sparsity of the counterfactual i.e. how many features are identical between $x$ and $x'$. A sparse counterfactual is much easier for people to reason about, and might be preferable to one with higher similarity, but lower sparsity. 

Aside from the three properties above, it can also be beneficial to consider if the counterfactual is an outlier based on the existing points in the target class or cluster, since a cluster can easily contain areas that lie far away from the points it contains, especially for centroid-based methods like k-means. We call this property plausibility, and it can be computed using off-the-shelf outlier scoring algorithms like the Local Outlier Factor (LOF) @breunigLOFIdentifyingDensitybased2000, which we use for our evaluations. Finally, some methods are also designed to return multiple counterfactuals for a single instance. In such cases, it is reasonable to consider the diversity of the counterfactuals returned by the method, since multiple highly similar counterfactuals offer no additional value for a practitioner. In @sec:experiments we introduce the concrete metrics used in this thesis to evaluate generated counterfactuals.

// Counterfactual usage for medicare application form error correction:@bhattExplainableMachineLearning2020


// fairness: select set of immutable features

== Unsupervised explanations

Although explainable AI is most frequently associated with supervised learning tasks like classification, it can also be used for unsupervised tasks where clustering is often the main objective. In general however, unsupervised explainability is less studied than supervised methods, although there has been some recent interest in the field of clustering explainability. The reason for this is that the inherent properties that group data points within a specific cluster are not explicitly defined, making it more challenging to identify these defining characteristics and, subsequently, to designate a clear target cluster for a counterfactual explanation.

The Iterative Mistake Minimization algorithm mentioned in @sec:properties_of_explanations @dasguptaExplainableKMeansKMedians2020 is one such example where the goal is to offer an explainable clustering using simple, human-interpretable rules to describe to which cluster a point belongs. In @ellisAlgorithmAgnosticExplainabilityUnsupervised2021, they present a model-agnostic method to explain any given clustering using feature importance found by permuting the data features and noting how much the clustering changes.

Due to the limited amount of research in this area, this thesis will focus primarily on studying and implementing methods for unsupervised explanations. Specifically, we focus on model-agnostic, post-hoc, counterfactual explanations as detailed in Sections #ref(<sec:properties_of_explanations>, supplement: none)-#ref(<sec:counterfactual_explanations>, supplement: none) applied to $k$-means clusterings. This is a specialized area with few existing methods and literature, most of which we explore in @sec:unsupervised_counterfactual_methods. When studying counterfactuals for clusterings, we define a counterfactual using an instance $x$ belonging to some cluster, along with a target cluster for the counterfactual $x'$. 

Counterfactuals for clusterings have many real-world use cases. For example, when applied to typical clustering tasks like customer segmentation or healthcare patient stratification, it can supply realistic and actionable ways to move a "low-value" customer to a more "high-value" cluster, or it can suggest lifestyle changes or treatments to move patients to a "healthy" cluster. Aside from this, as mentioned in @sec:counterfactual_explanations, counterfactuals are also useful simply because they offer a human-interpretable way for designers to understand their model or clustering.
