= Explainable AI
The field of machine learning (ML) has made great strides in recent years, resulting in much better classification accuracy and more complex models with potentially millions of parameters. Given the impressive performance of these models, industry has been quick to adopt them into their AI systems in production. 

However, these models are often used as black boxes: a tool which, when given an input, produces an output with no explanation or justification of how or why the model returned the given result. Furthermore, these advanced models have been proven to work in critical fields like cancer diagnosis or bank loan approval @bhattExplainableMachineLearning2020 @estevaDermatologistlevelClassificationSkin2017, both of which require a great deal of trust in the model in order to satisfy both end-users and stakeholders. For example, patients confronted with an AI-driven assessment of potentially malignant lesions would reasonably expect either an explanation of the model's conclusion or validation from a medical professional. Additionally, explainable AI (XAI) can also help combat bias and discrimination caused by AI systems, by investigating if the model is making decision based primarily on one or more sensitive attributes like age, gender or race. 

The problem of black-box AI systems has also recently led to legislation being passed by the EU as part of the AI act @EU_AI_Act_Article_86. In the legislation, article 86 ensures the "right to explanation" for individuals in the EU, when affected by the decision of a high-risk AI system, defined as systems acting in the area of biometrics, education, employment, law enforcement, etc. Here, the right to explanation means the affected subject can obtain a clear explanation of how AI was used in the decision, and the main points contributing to the decision. When the article goes into effect in 2026, deployers of AI systems will likely need to depend on XAI methods to comply with the new regulations.

Lastly, XAI is also interesting from a designer's perspective. By understanding how a model works, designers can better understand the strengths and weaknesses of their models, and potentially improve them. For example, one can use gradient-based explanation methods like Grad-CAM @selvarajuGradCAMVisualExplanations2020 for convolutional neural networks in order to understand which areas of an image contributed most to a classification (@grad_cam_example). In combination with domain knowledge, a designer can quickly use this to diagnose issues with a model. 

#figure(
    image("./assets/gradcam_example.png", width: 75%),
    caption: [Example of an explanation generated by Grad-CAM @selvarajuGradCAMVisualExplanations2020],
)<grad_cam_example>

== Properties of Explanations
Of course, there exists a large number of ways to explain ML models. These explanations can be broadly categorised using the following categories, based loosely on the taxonomy from @angelovExplainableArtificialIntelligence2021.

*Local vs. global explanations*. When seeking an explanation, one can either try to explain the entire model // FIND SOME GLOBAL METHOD EXAMPLE

// Local vs. Global
// Model-specific vs. Model-agnostic
// Post-hoc vs. Intrinsic

== Counterfactual Explanations

// Exogenous vs. Endogenous (Skin cancer example: CFs for uninterpretable features)
// Counterfactual usage for medicare application form error correction:@bhattExplainableMachineLearning2020

== Unsupervised Explanations

// TODO: Remove this
#pagebreak()
#bibliography("bibliography.bib", style: "springer-vancouver")
